# Database Switching Guide

This guide explains how to switch between different database backends in the cryptocurrency futures data collection system.

## Overview

The system supports **three database backends** that you can switch between by changing a single configuration option:

1. **SQLite** - Lightweight, file-based database
2. **Firebase** - Cloud-hosted realtime database
3. **PostgreSQL** - High-performance time-series database

## Quick Start

### Switching Databases

Edit `config.yaml` and change the `database_type`:

```yaml
# Choose: sqlite, firebase, or postgresql
database_type: "sqlite"
```

That's it! The system will automatically use the appropriate database backend.

## Database Options Comparison

| Feature | SQLite | Firebase | PostgreSQL |
|---------|--------|----------|------------|
| **Setup Time** | Instant | 5 minutes | 30 minutes |
| **Infrastructure** | None (file-based) | Zero (cloud) | Self-hosted |
| **Cost** | Free | $0-$20/month | VPS cost |
| **Best For** | Development | Prototyping | Production |
| **Data Limit** | Unlimited (disk) | 1GB free tier | Unlimited |
| **Performance** | Medium | Medium | Very High |
| **Queries** | SQL | NoSQL (JSON) | SQL + Time-series |
| **Real-time Sync** | No | Yes | No |
| **Scalability** | Limited | Auto-scaling | Manual scaling |

## 1. SQLite Setup (Recommended for Development)

### Advantages
- âœ… **Zero setup** - Just works out of the box
- âœ… **File-based** - Single database file
- âœ… **No server** - No PostgreSQL or Firebase needed
- âœ… **Portable** - Easy to copy and backup
- âœ… **SQL support** - Full SQL query capabilities

### Configuration

```yaml
database_type: "sqlite"

sqlite:
  database_path: "data/futures_data.db"  # Default location
```

### Usage

```bash
# Run unified collection script
python scripts/unified_historical_collection.py
```

The database file will be automatically created at `data/futures_data.db`.

### Database Size

For reference, typical database sizes:
- **1 symbol, 180 days**: ~50-100 MB
- **5 symbols, 180 days**: ~250-500 MB
- **10 symbols, 1 year**: ~1-2 GB

### Querying Data

```python
import sqlite3
import pandas as pd

# Connect to database
conn = sqlite3.connect('data/futures_data.db')

# Query OHLCV data
df = pd.read_sql_query("""
    SELECT * FROM ohlcv
    WHERE symbol = 'SOL/USDT'
    AND timeframe = '5m'
    ORDER BY time DESC
    LIMIT 1000
""", conn)

print(df.head())
conn.close()
```

### Optimizing SQLite

```python
from database.sqlite_manager import SQLiteManager

manager = SQLiteManager()
manager.initialize()

# Optimize database (reclaim space)
manager.vacuum()

# Check database size
size_mb = manager.get_database_size()
print(f"Database size: {size_mb:.2f} MB")
```

---

## 2. Firebase Setup (Recommended for Prototyping)

### Advantages
- âœ… **Zero infrastructure** - No server management
- âœ… **Cloud-hosted** - Access from anywhere
- âœ… **Real-time sync** - Live data updates
- âœ… **Free tier** - 1GB storage, 10GB/month bandwidth
- âœ… **Auto-scaling** - Handles traffic spikes

### Configuration

```yaml
database_type: "firebase"

firebase:
  service_account_path: "firebase-service-account.json"
  database_url: "https://your-project.firebaseio.com"
```

### Setup Steps

1. **Create Firebase Project**
   - Go to [Firebase Console](https://console.firebase.google.com/)
   - Create new project
   - Enable Realtime Database

2. **Download Service Account Key**
   - Project Settings â†’ Service Accounts
   - Generate new private key
   - Save as `firebase-service-account.json`

3. **Update Configuration**
   - Add service account path and database URL to `config.yaml`

4. **Run Collection**
   ```bash
   python scripts/unified_historical_collection.py
   ```

See [FIREBASE_SETUP.md](FIREBASE_SETUP.md) for detailed instructions.

---

## 3. PostgreSQL Setup (Recommended for Production)

### Advantages
- âœ… **Time-series optimized** - TimescaleDB extension
- âœ… **High performance** - Handles massive datasets
- âœ… **Complex queries** - Full SQL + analytics
- âœ… **Compression** - Automatic data compression
- âœ… **Production-ready** - Battle-tested at scale

### Configuration

```yaml
database_type: "postgresql"

database:
  host: "localhost"
  port: 5432
  database: "futures_db"
  user: "postgres"
  password: "your_password"
```

### Setup Steps

1. **Install PostgreSQL + TimescaleDB**
   ```bash
   sudo apt install postgresql postgresql-contrib
   sudo add-apt-repository ppa:timescale/timescaledb-ppa
   sudo apt update
   sudo apt install timescaledb-postgresql-14
   ```

2. **Create Database**
   ```bash
   sudo -u postgres psql
   CREATE DATABASE futures_db;
   \c futures_db
   CREATE EXTENSION IF NOT EXISTS timescaledb;
   \q
   ```

3. **Initialize Tables**
   ```bash
   python scripts/init_database.py
   ```

4. **Run Collection**
   ```bash
   # PostgreSQL has separate collectors
   python scripts/main_historical_collection.py
   ```

**Note:** PostgreSQL uses the original collector scripts, not the unified collector.

---

## Unified Collection Script

The `unified_historical_collection.py` script automatically adapts to your configured database:

```bash
python scripts/unified_historical_collection.py
```

**Output:**
```
================================================================================
ðŸ”¥ UNIFIED DATA COLLECTOR
================================================================================

ðŸ“Š Database Type: SQLITE
================================================================================

ðŸ“ Database Info:
  type: sqlite
  initialized: True
  database_path: data/futures_data.db
  size_mb: 0.0

ðŸ“… Collection Period:
  Start: 2023-05-15
  End: 2023-11-11
  Days: 180

ðŸ“Š Symbols: SOL/USDT
ðŸ“ˆ Timeframes: 5m, 15m, 1h, 4h, 1d
â° OI Periods: 5m, 15m, 1h
================================================================================

================================================================================
ðŸš€ [1/1] Starting collection for SOL/USDT
================================================================================

ðŸš€ Starting concurrent collection of 13 data streams for SOL/USDT...
ðŸ“Š Database: SQLITE
âœ… OHLCV 5m: 51840 records
âœ… OHLCV 15m: 17280 records
...
âš¡ Concurrent collection completed in 42.34s
ðŸ“Š Total records: 78,456

ðŸ“Š Collection Summary for SOL/USDT:
--------------------------------------------------------------------------------
  âœ… OHLCV_5m              :   51,840 records
  âœ… OHLCV_15m             :   17,280 records
  ...
--------------------------------------------------------------------------------
  ðŸ“ˆ Total Records: 78,456
  âœ… Successful: 13

================================================================================
ðŸŽ‰ COLLECTION COMPLETE
================================================================================

ðŸ“Š Overall Statistics:
  Total Symbols: 1
  Successful: 1
  Total Records: 78,456
  Database: SQLITE
  Database Size: 87.23 MB

âœ… Historical data collection completed!
================================================================================
```

---

## Switching Between Databases

### Example 1: Development â†’ Firebase

1. **Start with SQLite for local development:**
   ```yaml
   database_type: "sqlite"
   ```

2. **Collect initial data:**
   ```bash
   python scripts/unified_historical_collection.py
   ```

3. **Switch to Firebase for cloud deployment:**
   ```yaml
   database_type: "firebase"
   firebase:
     service_account_path: "firebase-service-account.json"
     database_url: "https://your-project.firebaseio.com"
   ```

4. **Re-run collection** (data goes to Firebase):
   ```bash
   python scripts/unified_historical_collection.py
   ```

### Example 2: Firebase â†’ PostgreSQL for Production

1. **Prototype with Firebase:**
   ```yaml
   database_type: "firebase"
   ```

2. **Scale up to PostgreSQL:**
   - Set up PostgreSQL server
   - Update configuration:
     ```yaml
     database_type: "postgresql"
     ```

3. **Migrate data** (optional):
   ```python
   # Export from Firebase
   firebase_df = await firebase_manager.get_ohlcv('SOL/USDT', '5m')

   # Import to PostgreSQL
   firebase_df.to_sql('ohlcv', postgresql_engine, if_exists='append')
   ```

---

## Configuration Examples

### Development Setup
```yaml
database_type: "sqlite"
sqlite:
  database_path: "data/dev_futures.db"
```

### Cloud Prototype
```yaml
database_type: "firebase"
firebase:
  service_account_path: "creds/firebase-dev.json"
  database_url: "https://crypto-futures-dev.firebaseio.com"
```

### Production Setup
```yaml
database_type: "postgresql"
database:
  host: "production-db.example.com"
  port: 5432
  database: "futures_prod"
  user: "futures_user"
  password: "${DB_PASSWORD}"  # Use environment variable
```

---

## Best Practices

### 1. Use SQLite for Local Development
- Fast iteration
- No setup required
- Easy debugging

### 2. Use Firebase for Prototypes
- Quick deployment
- Real-time capabilities
- Share data with team

### 3. Use PostgreSQL for Production
- Maximum performance
- Complex analytics
- Large datasets

### 4. Keep Configuration Flexible
Use environment variables for sensitive data:

```yaml
firebase:
  service_account_path: "${FIREBASE_SERVICE_ACCOUNT}"
  database_url: "${FIREBASE_DATABASE_URL}"
```

### 5. Backup Your Data
Regardless of database choice, implement backups:

**SQLite:**
```bash
cp data/futures_data.db backups/futures_data_$(date +%Y%m%d).db
```

**Firebase:**
```bash
# Export via Firebase Console or use Admin SDK
python scripts/export_firebase_data.py
```

**PostgreSQL:**
```bash
pg_dump futures_db > backups/futures_db_$(date +%Y%m%d).sql
```

---

## Troubleshooting

### "Configuration error: Invalid database_type"
**Solution:** Check that `database_type` is one of: `sqlite`, `firebase`, `postgresql`

### SQLite: "no such table: ohlcv"
**Solution:** The database is auto-initialized. If issues persist, delete the database file and re-run.

### Firebase: "service_account_path is required"
**Solution:** Ensure `service_account_path` and `database_url` are set in config.yaml

### PostgreSQL: "NotImplementedError"
**Solution:** PostgreSQL uses separate collectors. Use `scripts/main_historical_collection.py` instead.

---

## Performance Comparison

Benchmark: SOL/USDT, 180 days, 13 data streams

| Database | Collection Time | Storage Size | Query Speed |
|----------|----------------|--------------|-------------|
| SQLite | ~45s | 87 MB | Fast (indexed) |
| Firebase | ~45s | Cloud (N/A) | Fast (realtime) |
| PostgreSQL | ~30s | 45 MB (compressed) | Very Fast |

---

## Migration Tools

### SQLite â†’ Firebase
```python
from database.sqlite_manager import SQLiteManager
from database.firebase_manager import FirebaseManager

# Read from SQLite
sqlite_manager = SQLiteManager()
sqlite_manager.initialize()
df = await sqlite_manager.get_ohlcv('SOL/USDT', '5m')

# Write to Firebase
firebase_manager = FirebaseManager(...)
firebase_manager.initialize()
await firebase_manager.save_ohlcv_batch(df, 'SOL/USDT', '5m')
```

### Firebase â†’ SQLite
```python
# Read from Firebase
firebase_df = await firebase_manager.get_ohlcv('SOL/USDT', '5m')

# Write to SQLite
await sqlite_manager.save_ohlcv_batch(firebase_df, 'SOL/USDT', '5m')
```

---

## Summary

**Quick Reference:**

| Use Case | Database | Command |
|----------|----------|---------|
| Local development | SQLite | `database_type: "sqlite"` |
| Cloud prototype | Firebase | `database_type: "firebase"` |
| Production | PostgreSQL | `database_type: "postgresql"` |

**All use the same collection script:**
```bash
python scripts/unified_historical_collection.py
```

Switch anytime by changing `database_type` in `config.yaml`!
